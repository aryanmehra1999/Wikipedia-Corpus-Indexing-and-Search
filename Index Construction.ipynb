{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Document Ranking and Search\n",
    "### Aryan Mehra  (2017A7PS0077P)  ||  Siddhant Khandelwal (2017A7PS0127P)\n",
    "\n",
    "### Index Construction and Pre-processing\n",
    "We first import all the required files and python modules. They will aid the process of HTML file and data munging, extraction and later in the tokenization and processing of the textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options, punctuations and settings\n",
    "We now move on to declare the set of punctuations that will be removed from the corpus atthe time of document pre-processing and tokenization. Some other settings and declarations are shown as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "punctuations = ['.', ',', '!', '\\'', '\\\"', '(', ')', '[', ']', '{', '}', '?', '\\\\', '/', '~', '|', '<', '>']\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Frequency Distribution\n",
    "The ***build_vocabulary_freqdist*** function takes as an input the huge list of all words occuring in the corpus, and then proceeds to return a dictionary (data structure) that maps the unique words to their corpus frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_freqdist(vocabulary):\n",
    "    vocabulary = nltk.FreqDist(vocabulary)\n",
    "    vocabulary = sorted(vocabulary.items(), key=lambda x: x[0])\n",
    "    vocabulary = dict(vocabulary)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the vocabulary - Metadata 1\n",
    "The following ***stem_the_vocab*** function takes the vocabulary dictionary of the corpus and creates a metadata file that contains the stemmed vocabulary words as keys mapped to the highest occuring original word of it's equivalence class. \n",
    "***For example:*** If \"building\" and \"buildings\" occur 10 and 20 times respectively, and both are stemmed to \"build\", the stemmed vocabulary will contain the key \"build\" mapped to (\"buildings\", 20). We will use this metadata later for enhancing search of related words, in case the original is not in the vocabulary itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_the_vocab(vocabulary_dict):\n",
    "\n",
    "    vocabulary_keys = list(vocabulary_dict.keys())\n",
    "    vocabulary_values = list(vocabulary_dict.values())\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stemmed_vocab = {}\n",
    "\n",
    "    for word_id, word in enumerate(vocabulary_keys):\n",
    "        stemmed_word = porter.stem(word)\n",
    "        if stemmed_word not in stemmed_vocab:\n",
    "            stemmed_vocab[stemmed_word] = (word, vocabulary_values[word_id])\n",
    "        else:\n",
    "            if stemmed_vocab[porter.stem(word)][1] < vocabulary_values[word_id]:\n",
    "                stemmed_vocab[porter.stem(word)] = (word, vocabulary_values[word_id])\n",
    "\n",
    "    with open(\"stemmed_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(stemmed_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Database and Vocabulary Building - Parsing and Metadata 2 Creation\n",
    "\n",
    "The following is the function that goes through the text file containing hundreds of unprocessed HTML documents. The only input to the function is the filename of the file thats contains the corpus. It parses the document manually wthout the use of inbuilt libraries for a custom implementation and step bt step visualization of the ongoing process. We parse all the words that occur within 2 < /doc > tags. Hence only the content and headings are parsed. At the end, we save the raw vocabulary as well as extract and save the document titles from within the document tags. This will be later used by the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_database_vocabulary(filename):\n",
    "    with open(filename, 'r',encoding='utf8') as f:\n",
    "        text = f.read().replace('\\n', ' ')\n",
    "    database = []\n",
    "    vocabulary = []\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    doc_titles = []\n",
    "\n",
    "    while(i < len(text)):\n",
    "\n",
    "        if(text[i] == '<' and text[i+1] == 'd' and text[i+2] == 'o' and text[i+3] == 'c'):\n",
    "\n",
    "            # keep finding the end of the doc begin tag\n",
    "            flag = 0\n",
    "            title = \"\"\n",
    "\n",
    "            while(text[i] != '>'):\n",
    "                i = i+1\n",
    "                if(text[i-6] == 't' and text[i-5] == 'i' and text[i-4] == 't' and text[i-3] == 'l' and text[i-2] == 'e' and text[i-1] == '='):\n",
    "                    flag = 1\n",
    "\n",
    "                if(flag == 1 and text[i] != '>'):\n",
    "                    title = title + str(text[i])\n",
    "\n",
    "            doc_titles.append(title)\n",
    "\n",
    "            i = i + 2\n",
    "\n",
    "            document = ''\n",
    "\n",
    "            while(not (text[i] == '<' and text[i+1] == '/' and text[i+2] == 'd' and text[i+3] == 'o')):\n",
    "                document = document + str(text[i])\n",
    "                i = i+1\n",
    "\n",
    "            while(text[i] != '>'):\n",
    "                i = i+1\n",
    "\n",
    "            doc_count = doc_count + 1\n",
    "\n",
    "            if(doc_count % 200 == 0):\n",
    "                print(\"Document \" + str(doc_count) + \" is being read...\")\n",
    "\n",
    "            # pre-processing\n",
    "            clean_document = bsoup(document, 'html.parser').get_text()\n",
    "            clean_document = clean_document.lower()\n",
    "            # tokens is a list of tokens\n",
    "            tokens = nltk.word_tokenize(clean_document)\n",
    "            tokens = [token for token in tokens if token not in punctuations]\n",
    "\n",
    "            for token in tokens:\n",
    "                vocabulary.append(token)\n",
    "\n",
    "            # saving the file / document wise write-back\n",
    "            # database is a list of list, as mentioned above\n",
    "            database.append(tokens)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    print(\"Done with the document reading...Database is ready \\n\")\n",
    "    print(\"There are total \" + str(doc_count) + \" documents!\")\n",
    "    print(\"There are total \" + str(len(doc_titles)) + \" titles!\")\n",
    "\n",
    "    # vocabulary is a dictionary of words/tokens and their corpus frequencies\n",
    "    vocabulary = build_vocabulary_freqdist(vocabulary)\n",
    "\n",
    "    print(\"Done with the Vocabulary building...Vocab is ready and saved !\\n\")\n",
    "\n",
    "    with open(\"vocabulary_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "\n",
    "    with open(\"doc_titles.pkl\", \"wb\") as f:\n",
    "        pickle.dump(doc_titles, f)\n",
    "\n",
    "    stem_the_vocab(vocabulary)\n",
    "\n",
    "    # print(vocabulary)\n",
    "    return database, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Vector Creation\n",
    "The following ***build_documents_vector*** function takes in as input the database of documents, vocabulary words and the inverse vocabulary word dictionary (data structure that maps the word to it's index in the vocabulary in constant time access). It then creates the vector representation of the document and stores it as a numpy file, simply for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_vector(database, vocabulary_words, inverse_vocab_word_dict):\n",
    "    documents_vector = np.zeros((len(database), len(vocabulary_words)))\n",
    "\n",
    "    # populate the documents_vector with the frequency of each vocabulary word for each document\n",
    "    for doc_id, doc in enumerate(database):\n",
    "        for token in doc:\n",
    "            documents_vector[doc_id][inverse_vocab_word_dict[token]] = documents_vector[doc_id][inverse_vocab_word_dict[token]] + 1\n",
    "    \n",
    "    print(\"Done with the Documents Vector build... Saving it as numpy file \\n\")\n",
    "    np.save(\"documents_vector.npy\", documents_vector)\n",
    "    return documents_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document vector processing - LNC scheme\n",
    "The function is self explainatory in the sense that it uses the documents vector to process the document vector. The non zero values are taken a log. Then cosine normalisation is done manually by calculating the euclidean magnitude of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_vector(documents_vector):\n",
    "\n",
    "    for i in range(0, documents_vector.shape[0]):\n",
    "        for j in range(0, documents_vector.shape[1]):\n",
    "            if(documents_vector[i][j] > 0):\n",
    "                documents_vector[i][j] = 1 + math.log(documents_vector[i][j])\n",
    "\n",
    "    print(\"Done with the log calculation build... \\n\")\n",
    "\n",
    "    # calculation of cosine normalisation\n",
    "    temp_documents_vector = np.copy(documents_vector)\n",
    "    temp_documents_vector = np.square(temp_documents_vector)\n",
    "    temp_documents_vector = np.sum(temp_documents_vector, axis=1)\n",
    "    temp_documents_vector = np.sqrt(temp_documents_vector)\n",
    "    documents_vector = np.divide(documents_vector, temp_documents_vector[:, None])\n",
    "\n",
    "    print(\"Done with the cosine normalization build... \\n\")\n",
    "    return documents_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "This is the main function of the program that calls all the above mentioned required fucntionalities and saves the files. The function also builds the inverse vocabulary dictionary for faster constant time access later, but we don't save it as another metadata because it can anyway be constructed again from the vocabulary again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_construction(filename):\n",
    "    database, vocabulary = build_database_vocabulary(filename)\n",
    "    # how many unique words\n",
    "    vocabulary_words = list(vocabulary.keys())\n",
    "    inverse_vocab_word_dict = {k: v for v, k in enumerate(vocabulary_words)}\n",
    "    documents_vector = build_documents_vector(database, vocabulary_words, inverse_vocab_word_dict)\n",
    "    documents_vector = process_documents_vector(documents_vector)\n",
    "    np.save(\"database_lnc.npy\", documents_vector)\n",
    "    print(\"Saved! database_lnc.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating Sequence\n",
    "The filename of the corpus is to be put in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 200 is being read...\n",
      "Document 400 is being read...\n",
      "Document 600 is being read...\n",
      "Document 800 is being read...\n",
      "Done with the document reading...Database is ready \n",
      "\n",
      "There are total 963 documents!\n",
      "There are total 963 titles!\n",
      "Done with the Vocabulary building...Vocab is ready and saved !\n",
      "\n",
      "Done with the Documents Vector build... Saving it as numpy file \n",
      "\n",
      "Done with the log calculation build... \n",
      "\n",
      "Done with the cosine normalization build... \n",
      "\n",
      "Saved! database_lnc.npy\n"
     ]
    }
   ],
   "source": [
    "index_construction(\"mega_wiki_corpus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
